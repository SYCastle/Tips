# 1.機械学習の現状


## 1.1 機械学習とは何か

- コンピュータがデータから学習できるようにするためのコンピュータプログラミングについての科学


## 1.2 なぜ機械学習を使うのか

- 既存のソリューションでは、手作業による大量のチューニングや、ルールの長いリストが必要な問題に対して有効
    - ex.) スパムメールのフィルタを、機械学習を使わないで実現する場合、「～なタイトルの場合、またはリンクが書かれている～」とスパムの特徴をルールベースで大量に記述。メンテナンス性と正確性に欠ける。

## 1.3 機械学習システムのタイプ

大きく以下に分類される

- 人間の関与のもとで訓練されるかどうか(教師あり、教師なし、半教師あり、強化学習)
- その場で少しずつ学習できるかどうか(オンライン、バッチ学習)
- 単純に新しいデータポイントと既知のデータポイントを比較するか、科学者が行うように訓練データからパターンを見つけ出して予測モデルを構築するか(インスタンスベース、モデルベース学習)

### 1.3.1 教師あり/教師なし学習

#### 1.3.1.1 教師あり学習

- 教師あり機械学習のタスクとしてまず思いつくのが分類
    - ex.) スパムメールフィルタ。クラス付け(スパムかそうでないか)された大量のデータを訓練

- また、一連の特徴量(ex.走行距離、使用年数...etc)からターゲットのラベルである数値(ex.中古車の価格)を予測する回帰と呼ばれるタスクでも使われる


重要な教師あり学習アルゴリズムの一部を以下に挙げる

- k近傍法
- 線形回帰(4章)
- ロジスティクス回帰(4章)
- サポートベクターマシーン(5章)
- 決定木とランダムフォレスト(6,7章)
- ニューラルネットワーク(10章)

#### 1.3.1.2 教師なし学習

- 訓練データにラベルがついていない

重要な教師なし学習アルゴリズムの一部を以下に挙げる(詳細は8章等)

- クラスタリング(ex.ブログの訪問者について、類似する集団分けができる。40%の訪問者は漫画好きの男性で夜中にブログを読んでいる...etc)
    - k平均
    - 階層型クラスタ分析
    - EMアルゴリズム
- 可視化と次元削減(ex.可視化のアウトプットは、プロットしやすい2次元/3次元データであり、プロットすることでデータの分布等のパターンを目視できる。次元削減(情報を損なわず、データを単純化)も併用される)
    - 主成分分析(ex.車の走行距離は、使用年数と非常に高い相関を示すため、これら2つの情報を1つにまとめる)
    - カーネルPCA
    - LLE(Locally-Linier Embedding:局所線形埋め込み)
    - t-SNE(t-distributed stochastic neighbor embedding:t分布型確率的近傍埋め込み法)
- 相関ルール学習
    - アプリオリ
    - eclat

教師なし学習の重要なタスクとして、以下が挙げられる

- 異常検知
    - ex.異常なクレジットカード取引の発見
- 相関ルール学習(大量のデータを掘り下げて、属性の間から面白い関係を見つけ出す)
    - ex.スーパーマーケットの購買データから、「バーベキューソースとポテトチップスを買う人はステーキ肉も買っていることがわかるので、それらの商品を近くに配置する」etc

#### 1.3.1.3 半教師あり学習

- 一部だけラベルが付けられたデータを扱う
    - ex.Google Photosのように、アップロードした写真の人のが誰なのか分類/検索できる。まず教師なし学習(クラスタリング)により、人物Aは写真No1,5,11、人物Bは写真No2,5,7に写っていることが分かり、ラベル付き教師データ(人物A/Bがそれぞれ誰なのか)を提供されることで、写真に写っている人が誰なのか分かる。

ほとんどの半教師あり学習は、教師なし学習アルゴリズムと教師あり学習アルゴリズムを結合したもの。
ex.DBN(Deep Breaf Net)は制限付きボルツマンマシン(RBM)という教師なしコンポーネントを積み上げたもの。

#### 1.3.1.4 強化学習

- ex.囲碁 (AlphaGo)
- 学習システム(エージェント)は、環境を観察し、行動を選択して実行し、より良い報酬を得るように、方策(policy)と呼ばれる戦略を自分で学習していく

### 1.3.2 バッチ学習とオンライン学習

#### 1.3.2.1 バッチ学習

- 訓練するときに、すべての訓練データを与えるため、少しずつ学習せず、毎回0から～新しいデータセット含めて学習 & 更新
    - 特に、「デプロイ前に学習したことだけ」を利用する → オフライン学習と呼ばれる 


#### 1.3.2.2 オンライン学習

- ミニバッチと呼ばれる小さいなグループデータセットから、差分的に訓練する学習
    - 変化するデータにどれくらいの速さで対応するかを表す学習速度という重要な指標がある
        - 学習速度が速い：新しいデータセットにすぐ対応できるが、古いデータをすぐ忘れる傾向
        - 学習速度が遅い: 上記の逆

- 問題点：不良なデータを与えると、次第に性能が下がる
    - 性能低下の検出や、学習をオフする、といった枠組みが必要

### 1.3.3 インスタンスベース学習とモデルベース学習

汎化には「インスタンスベース学習とモデルベース学習」の二つの主要アプローチがある
※汎化...訓練データ以外の未知のデータが入力された際の対応能力。

#### 1.3.3.1 インスタンスベース学習

- 訓練データ例を丸暗記し、新しいデータに対しては、類似度(ex.スパムメールの例だと類似単語数)の尺度を使って汎化

#### 1.3.3.2 モデルベース学習

- 訓練データ例からモデルを構築し、そのモデルを使って予測する(ex.二次元データセットをぷろっとし、漸近線を引く)
    - モデルを使えるようにするには、パラメータ(モデルの変数の係数等)を定義する必要がある
        - どうすれば良いモデルが得られる? 「良い」とは？
            - →モデルがどれだけ悪いかを測定するコスト関数を使用する 

## 1.4 機械学習が抱える難関

### 1.4.1 訓練データ例の品質の低さ

小さい「りんごとは何か」を教えたい場合、りんごを指して「あれはりんご」と言えば済むが、
機械学習はそれほどの認識LVを持たず、ごく簡単な問題でも数千個のデータ例が必要。

### 1.4.2 現実を代表しているとは言えない訓練データ

ex.)1人あたりのGDPデータから、「お金によって人は幸せになるか」そのモデルを構築したとしても、インプットのデータにない国のデータをモデルに突っ込んでも、正確な予測は得られない

### 1.4.3 品質の低いデータ

訓練データ誤りや、外れ値、ノイズが多い等

### 1.4.4 無関係な特徴量

- 訓練のために適切な特徴量を揃えることが大切であり、このプロセスは特徴量エンジニアリングと呼ばれ、以下の作業で構成される
    - 特徴量選択：既存の特徴量から訓練に役立つ特徴量を選択する
    - 特徴量抽出：既存の特徴量を組みわせて、より役立つひとつの特徴量を作る(次元削減アルゴリズム等)
 
### 1.4.5 訓練データへの過学習

- 過学習 = 汎化性能が低い. ex.) 訓練データに対しては高い性能を示すが、新しいデータセットに対する性能が低い
    - 過学習は、訓練データの量やノイズの割合に比べてモデルが複雑すぎるときに起きる
        - 解決方法としては以下のようなものがあげられる
            - パラメータの少ないモデルを選択する、訓練データの中の特徴量を減らす、モデルに制約を与える(正規化)
            - より多くの訓練データを集める
            - 訓練データ内のノイズを減らす

### 1.4.6 訓練データへの過小適合

- 過学習の逆。モデルが単純すぎて、データの背景に隠れている構造を学習できていない状態。
    - 解決方法としては以下のようなものがあげられる
        - パラメータが多い強力なモデルを選ぶ
        - 特徴量をもっとよいものにする
        - モデルに対する制約をゆるめる(例えば、正規化ハイパーパラメータを小さくする)

## 1.5 テストと検証

- データに適切に汎化するかどうか確かめたい
    - 集めたデータを「訓練セット」と「テストセット」に分割し、訓練セットを使ってモデルを訓練し、テストセットを使ってモデルを評価する。
    ※新しいデータを与えたときの誤り率を汎化誤差、または標本誤差
    

- その他：テストセットのほかに検証セットを用意するケースがある(訓練セットで様々なハイパーパラメータで複数のモデルを訓練し、検証セットで最も高い性能を出したモデルとハイパーパラメータを選択し、最後にテストセットで評価する)

## 1.6 演習問題



